{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_TrainNeuralNetworkWithPytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maupin1991/ML_pytorch_tutorial/blob/master/4_TrainNeuralNetworkWithPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eF8S3nM-ksT",
        "colab_type": "text"
      },
      "source": [
        "# Training a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axSgjchU-prt",
        "colab_type": "text"
      },
      "source": [
        "Now we know all the objects to combine in order to build a neural network and to feed to it some input. We're ready for working at Google!\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "Not really...\n",
        "\n",
        "\n",
        "\n",
        "We forgot to train the network! It has to learn some mapping between the inputs and the desired output.\n",
        "* The first thing is to define a criterion for telling the network right from wrong. \n",
        "* After, we need to show the training set to the network and update its weights for improving its performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbyVPfjPEpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "np.random.seed(99)\n",
        "torch.manual_seed(10);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4p07s_PZBae",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPuRLAraZDGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model class\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        \n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.log_softmax(self.fc4(x), dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSjUFU80oZFs",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMYT8rBXGLpG",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the MNIST dataset. As we know, it contains 60k training images and 10k testing images. These are a lot of images, and we would have to wait a long time before seeing our training results. We can use the Subset class of torch for reducing the dataset size. Of course we are going to have less training data and our network will have \"less\" examples to see, but as we will see we can get to good accuracy even with this reduced set.\n",
        "\n",
        "The subset class takes as input the dataset to sample and the indices to use. We can easily generate (and **shuffle**) the indices with numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9n2h6EroYAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "n_train = 5000\n",
        "n_test = 1000\n",
        "# validation set is 20% of the training set\n",
        "valid_size = 0.2\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# Download and load the data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, \n",
        "                          train=True, transform=transform)\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, \n",
        "                          train=False, transform=transform)\n",
        "\n",
        "# Splitting train/validation and testing set\n",
        "\n",
        "# training set\n",
        "train_idxs = np.arange(len(trainset))\n",
        "np.random.shuffle(train_idxs)\n",
        "train_idxs = train_idxs[:n_train].tolist()\n",
        "\n",
        "# subsample validation set from training set\n",
        "n_valid = int(np.floor(n_train * valid_size))\n",
        "valid_idxs = train_idxs[:n_valid]\n",
        "train_idxs = train_idxs[n_valid:]\n",
        "\n",
        "# testing set\n",
        "test_idxs = np.arange(len(testset))\n",
        "np.random.shuffle(test_idxs)\n",
        "test_idxs = test_idxs[:n_test].tolist()\n",
        "\n",
        "# extract only the selected indices\n",
        "train_subset = torch.utils.data.Subset(trainset, train_idxs)\n",
        "valid_subset = torch.utils.data.Subset(trainset, valid_idxs)\n",
        "test_subset = torch.utils.data.Subset(testset, test_idxs)\n",
        "\n",
        "# data loader (finally)\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=32)\n",
        "validloader = torch.utils.data.DataLoader(valid_subset, batch_size=32)\n",
        "testloader = torch.utils.data.DataLoader(test_subset, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqKJOlbGPCCf",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvzD0OknPRYz",
        "colab_type": "text"
      },
      "source": [
        "The nn module defines specific loss classes that we can use to determine the error of our network.\n",
        "Read the documentation carefully. The cross entropy loss module combines a log-softmax layer and a negative-log-likelihood loss (useful for classification problems with C classes).\n",
        "\n",
        "This means we need to pass to this function the scores of the last layer, so we won't need the softmax layer in the end! Another option would be to separate the steps, including the softmax layer in the network, and using the NLLLoss module of the package nn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORY2_2oxPRhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N03VHxWGN7s3",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "We need to update the weights of the network towards the direction that gives us a smaller error. To do this, we use **gradient descent**, propagating the gradient of the loss backward through all layers of the network. \n",
        "\n",
        "As we propagate the error backwards, we update the layers' weights by subtracting the gradient of the loss w.r.t each weight multiplied by the **learning rate** .\n",
        "\n",
        "We will use **autograd**, a PyTorch module that calculates automatically the gradients of tensors. This module keeps track of all the operations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdaLdIWSN6-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.rand(1, 1, requires_grad=True)\n",
        "print(\"tensor x:\",x)\n",
        "print(\"x.requires_grad=\",x.requires_grad)\n",
        "print(\"gradient of x:\", x.grad)\n",
        "squared = x**2\n",
        "squared.backward()\n",
        "\n",
        "# derivative of x**2 is 2*x\n",
        "# notice the grad_fn attribute of the tensor\n",
        "print(\"manually computed gradient:\", x*2)\n",
        "print(\"autograd:\", x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn0Pr7SFnWUU",
        "colab_type": "text"
      },
      "source": [
        "## Loss + Autograd\n",
        "\n",
        "With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OohYbgkayqem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Network()\n",
        "images, labels = next(iter(trainloader))\n",
        "output = net(images)\n",
        "loss = criterion(output, labels)\n",
        "\n",
        "print('Before backward pass: \\n', net.fc1.weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('After backward pass: \\n', net.fc1.weight.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s9e-GnnpEW-",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LvCmHRcpHFx",
        "colab_type": "text"
      },
      "source": [
        "Since training networks is what PyTorch users do all the time, it would be better to not reinvent the wheel everytime. In torch there is a module called **optim**, which contains implementations of several optimizers. \n",
        "\n",
        "* **optim.SGD** is the most used. It's vanilla Stochastic Gradient Descent\n",
        "* **optim.Adam** allow us to set momentum, which we've seen is useful for avoiding local minima\n",
        "\n",
        "REMEMBER: the optimizer accumulates the gradients, so for each step we will have to \"cleanup\" before the next step. We can use **optimizer.zero_grad()**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCC5a4aM7v0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:  \", device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqrWaKGNZS4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-9gY209xg7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8uauv8ZyLCg",
        "colab_type": "text"
      },
      "source": [
        "## We're ready to train our network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkqAAHxV2dxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.to(device)\n",
        "train_losses, valid_losses = [], []\n",
        "\n",
        "# train model and evaluate with validation set\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        log_ps = net(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        valid_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            for images, labels in validloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                log_ps = net(images)\n",
        "                valid_loss += criterion(log_ps, labels)\n",
        "                \n",
        "                ps = torch.exp(log_ps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        valid_losses.append(valid_loss/len(validloader))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Validation Loss: {:.3f}.. \".format(valid_losses[-1]),\n",
        "              \"Validation Accuracy: {:.3f}\".format(accuracy/len(validloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK7eakwAqSUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate on testing set\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = net(images)\n",
        "        test_loss += criterion(output, labels)\n",
        "\n",
        "        top_p, top_class = output.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaDN12Mxq6Tm",
        "colab_type": "text"
      },
      "source": [
        "We have seen that everything works fine. But what happens if we try to add some noise to the images?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1sQZsxtXcMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Lambda(lambda x : x + torch.randn_like(x)/2)\n",
        "                              ])\n",
        "\n",
        "testset_noise = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, \n",
        "                          train=False, transform=transform)\n",
        "\n",
        "# testing set\n",
        "test_idxs = np.arange(len(testset_noise))\n",
        "np.random.shuffle(test_idxs)\n",
        "test_idxs = test_idxs[:n_test].tolist()\n",
        "\n",
        "test_subset = torch.utils.data.Subset(testset_noise, test_idxs)\n",
        "testloader_noisy = torch.utils.data.DataLoader(test_subset, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyHTrtFhX4mB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate on noisy set\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for images, labels in testloader_noisy:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = net(images)\n",
        "        test_loss += criterion(output, labels)\n",
        "\n",
        "        top_p, top_class = output.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    print(\"Test Noise Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZCwBIRwrOq7",
        "colab_type": "text"
      },
      "source": [
        "We can see that accuracy has dropped. Why did this happen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNli1A3v27UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_losses[1:], label='Training loss')\n",
        "plt.plot(valid_losses, label='Validation loss')\n",
        "plt.legend(frameon=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qtajZ_XKgOo",
        "colab_type": "text"
      },
      "source": [
        "We see that the training loss and the testing loss are diverging. This is a clear sign of overfitting. This means that the model is not able to generalize when new **unseen** data are presented as input.\n",
        "\n",
        "We can try to reduce this gap by applying some regularization technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmWm0FX33Br6",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynMZkF5Y3DRP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E86NGY8d3Dda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DropoutNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.1 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.dropout(torch.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = torch.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kszDU6Kw9TIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !!! Use dropout !!!\n",
        "net = DropoutNetwork()\n",
        "\n",
        "# !!! Add weight decay to the optimizer !!!\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.03, weight_decay=0.01)\n",
        "\n",
        "net.to(device)\n",
        "train_losses, valid_losses = [], []\n",
        "\n",
        "# train model and evaluate with validation set\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        log_ps = net(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        valid_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            for images, labels in validloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                log_ps = net(images)\n",
        "                valid_loss += criterion(log_ps, labels)\n",
        "                \n",
        "                ps = torch.exp(log_ps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        valid_losses.append(valid_loss/len(validloader))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Validation Loss: {:.3f}.. \".format(valid_losses[-1]),\n",
        "              \"Validation Accuracy: {:.3f}\".format(accuracy/len(validloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc22xPj5rX-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate on testing set\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = net(images)\n",
        "        test_loss += criterion(output, labels)\n",
        "\n",
        "        top_p, top_class = output.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2hFCNGRY7cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate on noisy set\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for images, labels in testloader_noisy:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = net(images)\n",
        "        test_loss += criterion(output, labels)\n",
        "\n",
        "        top_p, top_class = output.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    print(\"Test Noise Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRLTuOYMKYkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(valid_losses, label='Validation loss')\n",
        "plt.legend(frameon=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OCGFj1k7SUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_classes = 10\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(testloader):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9irDTzP-3s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x = confusion_matrix.numpy()\n",
        "from pandas import DataFrame\n",
        "print(DataFrame(x))\n",
        "plt.imshow(confusion_matrix.numpy());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbV-y1hk9pZ7",
        "colab_type": "text"
      },
      "source": [
        "## Use the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMOWt89z9rs0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfM_y8g431hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "classes = range(10)\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "images_cuda = images.to(device)\n",
        "\n",
        "# get sample outputs\n",
        "output = net(images_cuda)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.cpu().numpy())\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in range(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(images[idx][0, :, :], interpolation='nearest', cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}